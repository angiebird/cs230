{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "from rnn_model import *\n",
    "import cv2\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional_recurrent import ConvLSTM2D\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.utils.np_utils import to_categorical   \n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv3D\n",
    "\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "# I/O libraries\n",
    "import os\n",
    "import tarfile\n",
    "import tempfile\n",
    "from six.moves import urllib\n",
    "\n",
    "# Helper libraries\n",
    "import random\n",
    "import matplotlib\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2 as cv\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm\n",
    "\n",
    "from jupyter_nb import seg_elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Many-To-Many Rnn network\n",
    "# X: 5D tensor with shape: (samples, time, rows, cols, channels)\n",
    "# Y: 5D tensor with shape: (samples, time, output_row, output_col, filters=num_classes)\n",
    "\n",
    "def gen_data(main_dir, batch_size, num_frames=10, INPUT_SHAPE=(360, 640)):\n",
    "    i = 0\n",
    "    start_frames = range(0, 5000, batch_size)\n",
    "    while True:\n",
    "        input_batch = []\n",
    "        output_batch = []\n",
    "        for b in range(batch_size):\n",
    "            if i == len(start_frames):\n",
    "                i = 0\n",
    "                random.shuffle(start_frames)\n",
    "            input_imgs = []\n",
    "            output_imgs = []\n",
    "            # print(start_frames[i])\n",
    "            for k in range(num_frames):\n",
    "                input_file = os.path.join(main_dir, 'infer', '00{:04d}_if_id.png'.format(start_frames[i+k]))\n",
    "                gt_file = os.path.join(main_dir, 'corrected_gt_id', '00{:04d}_gt_id.png'.format(start_frames[i+k]))\n",
    "                # print(input_file, \" \", gt_file)\n",
    "                infer_img = cv2.resize(cv2.imread(input_file, cv2.IMREAD_GRAYSCALE), INPUT_SHAPE)[..., np.newaxis]\n",
    "                gt_img = cv2.resize(cv2.imread(gt_file, cv2.IMREAD_GRAYSCALE), INPUT_SHAPE)[..., np.newaxis]\n",
    "                # print(infer_img.shape)\n",
    "                input_imgs.append((infer_img.astype(float) - 10) / 10)\n",
    "                output_imgs.append((gt_img.astype(float) - 10) / 10)\n",
    "            i += 1\n",
    "            input_batch.append(input_imgs)\n",
    "            output_batch.append(output_imgs)\n",
    "        input_batch = np.array(input_batch)\n",
    "        output_batch = np.array(output_batch)\n",
    "        print(input_batch.shape, ' ', output_batch.shape)\n",
    "        yield input_batch, output_batch   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_conv_lstm_model(height, width, num_channels, num_classes):\n",
    "    seq = Sequential()\n",
    "    seq.add(ConvLSTM2D(filters=40, kernel_size=(3,3),\n",
    "                       input_shape=(None, height, width, num_channels),\n",
    "                       padding='same', data_format='channels_last',\n",
    "                       return_sequences=True))\n",
    "    seq.add(BatchNormalization())   \n",
    "    seq.add(ConvLSTM2D(filters=40, kernel_size=(3,3),\n",
    "                       padding='same',\n",
    "                       return_sequences=True))\n",
    "    seq.add(BatchNormalization())  \n",
    "    seq.add(ConvLSTM2D(filters=num_classes, kernel_size=(3, 3),                                                                                                                             \n",
    "                       padding='same', return_sequences=True)) \n",
    "    seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
    "                       padding='same', return_sequences=True))\n",
    "    seq.add(BatchNormalization())\n",
    "    seq.add(Conv3D(filters=20, kernel_size=(3, 3, 3),\n",
    "                   activation='softmax',\n",
    "                   padding='same', data_format='channels_last'))    \n",
    "#    run_opts = tf.RunOptions(report_tensor_allocations_upon_oom = True)\n",
    "    # opt = Adam(lr=0.05, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "    seq.compile(\n",
    "        optimizer='adam',\n",
    "        # optimizer=opt,\n",
    "        # loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        # loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy'])\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_mit_data(ds_dir='/home/ubuntu/mit_driveseg_dataset/npz_files', \n",
    "                 batch_size=1,\n",
    "                 start_idx = 1,\n",
    "                 dataset_size=100):\n",
    "    batchs = list(range(start_idx, dataset_size+start_idx))\n",
    "    num_examples = 10\n",
    "    while True:\n",
    "        # print('one epoch')\n",
    "        random.shuffle(batchs)\n",
    "        print(batchs)\n",
    "        for batch_idx in batchs:\n",
    "            data = np.load(os.path.join(ds_dir, 'ds{:04d}.npz'.format(batch_idx)))\n",
    "            num_frames = data['X'].shape[0]\n",
    "            print(int(num_frames/num_examples))\n",
    "            X = data['X'].reshape((num_examples, int(num_frames/num_examples), 360, 640, 22))\n",
    "            Y = data['Y'].reshape((num_examples, int(num_frames/num_examples), 360, 640, 1))\n",
    "            data.close()\n",
    "            print(X.shape, ' ', Y.shape) \n",
    "        #print(Y[0,0,:10,:10,::])\n",
    "            Y = to_categorical(Y, num_classes=20)\n",
    "        #print(X.shape, ' ', Y.shape) \n",
    "        #print(Y[0,0,:10,:10,::])\n",
    "            k = list(range(num_examples//batch_size))\n",
    "            random.shuffle(k)\n",
    "            for kk in k:\n",
    "                yield X[kk*batch_size:(kk+1)*batch_size], Y[kk*batch_size:(kk+1)*batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the reshape is correct.\n",
    "ds_dir='/home/ubuntu/mit_driveseg_dataset/npz_files'\n",
    "data = np.load(os.path.join(ds_dir, 'ds{:04d}.npz'.format(1)))\n",
    "print(data['X'].shape)\n",
    "Y = data['Y'].reshape((1, 1, 360, 640, 1))\n",
    "print(Y.shape)\n",
    "print(data['X'].reshape((1, 1, 360, 640, 22)).shape)\n",
    "#print(np.array_equal(Y[1, 3],data['Y'][8]))\n",
    "print(data['X'][0,:10,:10,:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_lst_m2d_4 (ConvLSTM2D)  (None, None, 360, 640, 40 89440     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, None, 360, 640, 40 160       \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_5 (ConvLSTM2D)  (None, None, 360, 640, 40 115360    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, None, 360, 640, 40 160       \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_6 (ConvLSTM2D)  (None, None, 360, 640, 20 43280     \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_7 (ConvLSTM2D)  (None, None, 360, 640, 40 86560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, None, 360, 640, 40 160       \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, None, 360, 640, 20 21620     \n",
      "=================================================================\n",
      "Total params: 356,740\n",
      "Trainable params: 356,500\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# for input_batch, output_batch in gen_data('/home/ubuntu/mit_driveseg_dataset', 10):\n",
    "#     print (input_batch.shape, \" \", output_batch.shape)\n",
    "    \n",
    "seq = build_conv_lstm_model(360, 640, 22, 20)\n",
    "seq.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Epoch 1/40\n",
      "[38, 40, 43, 44, 54, 37, 6, 5, 41, 3, 35, 51, 58, 46, 19, 16, 22, 4, 8, 9, 57, 23, 7, 45, 2, 10, 52, 1, 14, 26, 55, 29, 36, 33, 21, 13, 49, 34, 24, 20, 11, 27, 32, 25, 53, 39, 17, 12, 30, 50, 47, 60, 59, 48, 28, 31, 42, 56, 15, 18]\n",
      "5\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "5\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      " 9/60 [===>..........................] - ETA: 5:19 - loss: 2.5763 - acc: 0.62265\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "19/60 [========>.....................] - ETA: 3:33 - loss: 2.4220 - acc: 0.72225\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "29/60 [=============>................] - ETA: 2:31 - loss: 2.4055 - acc: 0.71635\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "39/60 [==================>...........] - ETA: 1:39 - loss: 2.3865 - acc: 0.72445\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "51/60 [========================>.....] - ETA: 41s - loss: 2.3665 - acc: 0.73705\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "60/60 [==============================] - 275s 5s/step - loss: 2.3434 - acc: 0.7567\n",
      "Epoch 2/40\n",
      " 1/60 [..............................] - ETA: 4:18 - loss: 2.4754 - acc: 0.60255\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      " 9/60 [===>..........................] - ETA: 3:39 - loss: 2.3773 - acc: 0.70325\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "21/60 [=========>....................] - ETA: 2:48 - loss: 2.3425 - acc: 0.73795\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "31/60 [==============>...............] - ETA: 2:04 - loss: 2.3516 - acc: 0.72895\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "39/60 [==================>...........] - ETA: 1:30 - loss: 2.3244 - acc: 0.75615\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "49/60 [=======================>......] - ETA: 47s - loss: 2.3203 - acc: 0.75995\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "59/60 [============================>.] - ETA: 4s - loss: 2.3165 - acc: 0.76365\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "60/60 [==============================] - 258s 4s/step - loss: 2.3154 - acc: 0.7647\n",
      "Epoch 3/40\n",
      "11/60 [====>.........................] - ETA: 3:30 - loss: 2.2411 - acc: 0.83735\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "21/60 [=========>....................] - ETA: 2:47 - loss: 2.2596 - acc: 0.81895\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "31/60 [==============>...............] - ETA: 2:04 - loss: 2.2670 - acc: 0.81165\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "41/60 [===================>..........] - ETA: 1:21 - loss: 2.2383 - acc: 0.84055\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "51/60 [========================>.....] - ETA: 38s - loss: 2.2560 - acc: 0.82285\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "60/60 [==============================] - 258s 4s/step - loss: 2.2527 - acc: 0.8260\n",
      "Epoch 4/40\n",
      " 1/60 [..............................] - ETA: 4:18 - loss: 2.2451 - acc: 0.83305\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      " 9/60 [===>..........................] - ETA: 3:39 - loss: 2.2207 - acc: 0.85745\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "21/60 [=========>....................] - ETA: 2:48 - loss: 2.2086 - acc: 0.86975\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "31/60 [==============>...............] - ETA: 2:04 - loss: 2.2198 - acc: 0.85865\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "41/60 [===================>..........] - ETA: 1:21 - loss: 2.2368 - acc: 0.84165\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "51/60 [========================>.....] - ETA: 38s - loss: 2.2317 - acc: 0.84665\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "60/60 [==============================] - 258s 4s/step - loss: 2.2319 - acc: 0.8464\n",
      "Epoch 5/40\n",
      " 1/60 [..............................] - ETA: 4:17 - loss: 2.2004 - acc: 0.87795\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "11/60 [====>.........................] - ETA: 3:31 - loss: 2.2132 - acc: 0.86525\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "21/60 [=========>....................] - ETA: 2:48 - loss: 2.2421 - acc: 0.83625\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "31/60 [==============>...............] - ETA: 2:04 - loss: 2.2370 - acc: 0.84145\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "41/60 [===================>..........] - ETA: 1:21 - loss: 2.2293 - acc: 0.84915\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "51/60 [========================>.....] - ETA: 38s - loss: 2.2197 - acc: 0.85875\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "60/60 [==============================] - 258s 4s/step - loss: 2.2310 - acc: 0.8474\n",
      "Epoch 6/40\n",
      " 1/60 [..............................] - ETA: 4:20 - loss: 2.2681 - acc: 0.81015\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "11/60 [====>.........................] - ETA: 3:31 - loss: 2.2601 - acc: 0.81815\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "21/60 [=========>....................] - ETA: 2:48 - loss: 2.2685 - acc: 0.80975\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "31/60 [==============>...............] - ETA: 2:04 - loss: 2.2468 - acc: 0.83155\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "41/60 [===================>..........] - ETA: 1:21 - loss: 2.2370 - acc: 0.84135\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "51/60 [========================>.....] - ETA: 38s - loss: 2.2378 - acc: 0.84055\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "60/60 [==============================] - 258s 4s/step - loss: 2.2255 - acc: 0.8528\n",
      "Epoch 7/40\n",
      " 1/60 [..............................] - ETA: 4:16 - loss: 2.2439 - acc: 0.83435\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "11/60 [====>.........................] - ETA: 3:30 - loss: 2.2371 - acc: 0.84105\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "21/60 [=========>....................] - ETA: 2:47 - loss: 2.2205 - acc: 0.85785\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "31/60 [==============>...............] - ETA: 2:04 - loss: 2.2361 - acc: 0.84225\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "41/60 [===================>..........] - ETA: 1:21 - loss: 2.2283 - acc: 0.85005\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "51/60 [========================>.....] - ETA: 38s - loss: 2.2245 - acc: 0.85375\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "60/60 [==============================] - 258s 4s/step - loss: 2.2351 - acc: 0.8431\n",
      "Epoch 8/40\n",
      " 1/60 [..............................] - ETA: 4:16 - loss: 2.2252 - acc: 0.85305\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "11/60 [====>.........................] - ETA: 3:30 - loss: 2.2079 - acc: 0.87035\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "21/60 [=========>....................] - ETA: 2:47 - loss: 2.2178 - acc: 0.86045\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "31/60 [==============>...............] - ETA: 2:04 - loss: 2.2228 - acc: 0.85545\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "41/60 [===================>..........] - ETA: 1:21 - loss: 2.2214 - acc: 0.85685\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "51/60 [========================>.....] - ETA: 38s - loss: 2.2128 - acc: 0.86535\n",
      "52/60 [=========================>....] - ETA: 34s - loss: 2.2122 - acc: 0.8660(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "60/60 [==============================] - 257s 4s/step - loss: 2.2063 - acc: 0.8718\n",
      "Epoch 9/40\n",
      " 1/60 [..............................] - ETA: 4:16 - loss: 2.3175 - acc: 0.76065\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "11/60 [====>.........................] - ETA: 3:30 - loss: 2.2751 - acc: 0.80305\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "21/60 [=========>....................] - ETA: 2:47 - loss: 2.2411 - acc: 0.83705\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "31/60 [==============>...............] - ETA: 2:04 - loss: 2.2464 - acc: 0.83185\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "41/60 [===================>..........] - ETA: 1:21 - loss: 2.2366 - acc: 0.84155\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "51/60 [========================>.....] - ETA: 38s - loss: 2.2276 - acc: 0.85065\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "60/60 [==============================] - 257s 4s/step - loss: 2.2226 - acc: 0.8555\n",
      "Epoch 10/40\n",
      " 1/60 [..............................] - ETA: 4:16 - loss: 2.2768 - acc: 0.80135\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "11/60 [====>.........................] - ETA: 3:29 - loss: 2.2570 - acc: 0.82125\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "21/60 [=========>....................] - ETA: 2:47 - loss: 2.2457 - acc: 0.83255\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "31/60 [==============>...............] - ETA: 2:04 - loss: 2.2235 - acc: 0.85475\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "41/60 [===================>..........] - ETA: 1:21 - loss: 2.2166 - acc: 0.86165\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "49/60 [=======================>......] - ETA: 47s - loss: 2.2023 - acc: 0.8758[45, 31, 2, 47, 9, 54, 16, 38, 4, 25, 22, 43, 41, 32, 55, 1, 14, 58, 10, 40, 11, 46, 36, 24, 8, 35, 29, 33, 6, 26, 28, 15, 27, 20, 48, 17, 3, 56, 18, 51, 52, 44, 49, 37, 34, 30, 60, 5, 53, 7, 12, 42, 23, 21, 50, 19, 39, 57, 59, 13]\n",
      "5\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "59/60 [============================>.] - ETA: 4s - loss: 2.2010 - acc: 0.87715\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "60/60 [==============================] - 257s 4s/step - loss: 2.2005 - acc: 0.8777\n",
      "Epoch 11/40\n",
      " 9/60 [===>..........................] - ETA: 3:38 - loss: 2.1911 - acc: 0.88715\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "19/60 [========>.....................] - ETA: 2:55 - loss: 2.2167 - acc: 0.86145\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "29/60 [=============>................] - ETA: 2:12 - loss: 2.2118 - acc: 0.86645\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "41/60 [===================>..........] - ETA: 1:21 - loss: 2.2243 - acc: 0.85395\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "49/60 [=======================>......] - ETA: 47s - loss: 2.2206 - acc: 0.85755\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "60/60 [==============================] - 257s 4s/step - loss: 2.2165 - acc: 0.8616\n",
      "Epoch 12/40\n",
      " 1/60 [..............................] - ETA: 4:17 - loss: 2.1354 - acc: 0.94365\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      " 9/60 [===>..........................] - ETA: 3:39 - loss: 2.1264 - acc: 0.95215\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "19/60 [========>.....................] - ETA: 2:56 - loss: 2.1529 - acc: 0.92545\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "29/60 [=============>................] - ETA: 2:12 - loss: 2.1637 - acc: 0.91455\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "41/60 [===================>..........] - ETA: 1:21 - loss: 2.1764 - acc: 0.90185\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "51/60 [========================>.....] - ETA: 38s - loss: 2.1944 - acc: 0.88395\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "59/60 [============================>.] - ETA: 4s - loss: 2.1976 - acc: 0.88065\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "60/60 [==============================] - 257s 4s/step - loss: 2.1975 - acc: 0.8807\n",
      "Epoch 13/40\n",
      " 9/60 [===>..........................] - ETA: 3:38 - loss: 2.1999 - acc: 0.87825\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "19/60 [========>.....................] - ETA: 2:55 - loss: 2.1932 - acc: 0.88495\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "29/60 [=============>................] - ETA: 2:13 - loss: 2.1989 - acc: 0.87925\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "39/60 [==================>...........] - ETA: 1:30 - loss: 2.1973 - acc: 0.88085\n",
      "40/60 [===================>..........] - ETA: 1:25 - loss: 2.1966 - acc: 0.8816(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "49/60 [=======================>......] - ETA: 47s - loss: 2.1877 - acc: 0.89055\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "60/60 [==============================] - 257s 4s/step - loss: 2.1892 - acc: 0.8890\n",
      "Epoch 14/40\n",
      " 1/60 [..............................] - ETA: 4:16 - loss: 2.2714 - acc: 0.80665\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      " 9/60 [===>..........................] - ETA: 3:38 - loss: 2.2433 - acc: 0.83475\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "21/60 [=========>....................] - ETA: 2:47 - loss: 2.2328 - acc: 0.84545\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "29/60 [=============>................] - ETA: 2:12 - loss: 2.2316 - acc: 0.84655\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "39/60 [==================>...........] - ETA: 1:29 - loss: 2.2384 - acc: 0.83975\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "49/60 [=======================>......] - ETA: 47s - loss: 2.2280 - acc: 0.85025\n",
      "50/60 [========================>.....] - ETA: 42s - loss: 2.2265 - acc: 0.8516(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "60/60 [==============================] - 257s 4s/step - loss: 2.2303 - acc: 0.8479\n",
      "Epoch 15/40\n",
      " 1/60 [..............................] - ETA: 4:17 - loss: 2.3834 - acc: 0.69485\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      " 9/60 [===>..........................] - ETA: 3:38 - loss: 2.2534 - acc: 0.82475\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "19/60 [========>.....................] - ETA: 2:55 - loss: 2.2206 - acc: 0.85765\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "31/60 [==============>...............] - ETA: 2:04 - loss: 2.2513 - acc: 0.82695\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "39/60 [==================>...........] - ETA: 1:29 - loss: 2.2322 - acc: 0.84605\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "49/60 [=======================>......] - ETA: 47s - loss: 2.2306 - acc: 0.84765\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "59/60 [============================>.] - ETA: 4s - loss: 2.2369 - acc: 0.84145\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "60/60 [==============================] - 257s 4s/step - loss: 2.2376 - acc: 0.8406\n",
      "Epoch 16/40\n",
      " 9/60 [===>..........................] - ETA: 3:37 - loss: 2.2648 - acc: 0.81355\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "19/60 [========>.....................] - ETA: 2:55 - loss: 2.2023 - acc: 0.87605\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "29/60 [=============>................] - ETA: 2:12 - loss: 2.2204 - acc: 0.85785\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "39/60 [==================>...........] - ETA: 1:29 - loss: 2.2164 - acc: 0.86185\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "51/60 [========================>.....] - ETA: 38s - loss: 2.2181 - acc: 0.86025\n",
      "52/60 [=========================>....] - ETA: 34s - loss: 2.2169 - acc: 0.8614(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "59/60 [============================>.] - ETA: 4s - loss: 2.2101 - acc: 0.86815\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "60/60 [==============================] - 257s 4s/step - loss: 2.2088 - acc: 0.8695\n",
      "Epoch 17/40\n",
      " 9/60 [===>..........................] - ETA: 3:38 - loss: 2.1858 - acc: 0.89245\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "21/60 [=========>....................] - ETA: 2:47 - loss: 2.1893 - acc: 0.88895\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "29/60 [=============>................] - ETA: 2:12 - loss: 2.1910 - acc: 0.88725\n",
      "30/60 [==============>...............] - ETA: 2:08 - loss: 2.1928 - acc: 0.8853(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "41/60 [===================>..........] - ETA: 1:21 - loss: 2.1882 - acc: 0.89005\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "49/60 [=======================>......] - ETA: 47s - loss: 2.1865 - acc: 0.89175\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "60/60 [==============================] - 257s 4s/step - loss: 2.1857 - acc: 0.8924\n",
      "Epoch 18/40\n",
      " 1/60 [..............................] - ETA: 4:18 - loss: 2.1911 - acc: 0.88705\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      " 9/60 [===>..........................] - ETA: 3:39 - loss: 2.1803 - acc: 0.89785\n",
      "10/60 [====>.........................] - ETA: 3:34 - loss: 2.1794 - acc: 0.8988(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "19/60 [========>.....................] - ETA: 2:56 - loss: 2.1723 - acc: 0.90585\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "29/60 [=============>................] - ETA: 2:13 - loss: 2.1829 - acc: 0.89525\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "41/60 [===================>..........] - ETA: 1:21 - loss: 2.2083 - acc: 0.86985\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "49/60 [=======================>......] - ETA: 47s - loss: 2.2084 - acc: 0.86975\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "60/60 [==============================] - 258s 4s/step - loss: 2.2115 - acc: 0.8667\n",
      "Epoch 19/40\n",
      " 1/60 [..............................] - ETA: 4:17 - loss: 2.2194 - acc: 0.85875\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      " 9/60 [===>..........................] - ETA: 3:39 - loss: 2.2001 - acc: 0.87805\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "19/60 [========>.....................] - ETA: 2:56 - loss: 2.1989 - acc: 0.87935\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "31/60 [==============>...............] - ETA: 2:04 - loss: 2.1979 - acc: 0.88025\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "41/60 [===================>..........] - ETA: 1:21 - loss: 2.2005 - acc: 0.87765\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "49/60 [=======================>......] - ETA: 47s - loss: 2.2171 - acc: 0.86105\n",
      "51/60 [========================>.....] - ETA: 38s - loss: 2.2208 - acc: 0.8573(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "60/60 [==============================] - 258s 4s/step - loss: 2.2256 - acc: 0.8525\n",
      "Epoch 20/40\n",
      " 1/60 [..............................] - ETA: 4:19 - loss: 2.2423 - acc: 0.83595\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "11/60 [====>.........................] - ETA: 3:31 - loss: 2.2177 - acc: 0.86075\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "21/60 [=========>....................] - ETA: 2:48 - loss: 2.2418 - acc: 0.83655\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "31/60 [==============>...............] - ETA: 2:05 - loss: 2.2251 - acc: 0.85315\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "41/60 [===================>..........] - ETA: 1:21 - loss: 2.2190 - acc: 0.85925\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "49/60 [=======================>......] - ETA: 47s - loss: 2.2171 - acc: 0.8610[10, 53, 23, 1, 33, 7, 31, 13, 20, 39, 45, 8, 42, 54, 52, 60, 17, 14, 15, 59, 50, 57, 48, 55, 43, 3, 2, 38, 34, 37, 16, 35, 32, 24, 4, 56, 51, 5, 19, 44, 6, 26, 21, 22, 29, 25, 12, 49, 40, 27, 46, 41, 9, 30, 47, 28, 18, 36, 58, 11]\n",
      "5\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "59/60 [============================>.] - ETA: 4s - loss: 2.2072 - acc: 0.87105\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "60/60 [==============================] - 258s 4s/step - loss: 2.2060 - acc: 0.8721\n",
      "Epoch 21/40\n",
      " 9/60 [===>..........................] - ETA: 3:39 - loss: 2.2606 - acc: 0.81755\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "19/60 [========>.....................] - ETA: 2:56 - loss: 2.2268 - acc: 0.85135\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "29/60 [=============>................] - ETA: 2:13 - loss: 2.2422 - acc: 0.83595\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "39/60 [==================>...........] - ETA: 1:30 - loss: 2.2272 - acc: 0.85095\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "51/60 [========================>.....] - ETA: 38s - loss: 2.2200 - acc: 0.85825\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "59/60 [============================>.] - ETA: 4s - loss: 2.2257 - acc: 0.85255\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "60/60 [==============================] - 258s 4s/step - loss: 2.2258 - acc: 0.8523\n",
      "Epoch 22/40\n",
      " 9/60 [===>..........................] - ETA: 3:39 - loss: 2.2418 - acc: 0.83635\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "19/60 [========>.....................] - ETA: 2:56 - loss: 2.1984 - acc: 0.87985\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "29/60 [=============>................] - ETA: 2:13 - loss: 2.1978 - acc: 0.88035\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "39/60 [==================>...........] - ETA: 1:30 - loss: 2.1997 - acc: 0.87855\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "49/60 [=======================>......] - ETA: 47s - loss: 2.1985 - acc: 0.87965\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "59/60 [============================>.] - ETA: 4s - loss: 2.2020 - acc: 0.87615\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "60/60 [==============================] - 258s 4s/step - loss: 2.2021 - acc: 0.8760\n",
      "Epoch 23/40\n",
      " 9/60 [===>..........................] - ETA: 3:39 - loss: 2.1760 - acc: 0.90215\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "19/60 [========>.....................] - ETA: 2:56 - loss: 2.1850 - acc: 0.89315\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "29/60 [=============>................] - ETA: 2:13 - loss: 2.1833 - acc: 0.89485\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "39/60 [==================>...........] - ETA: 1:30 - loss: 2.1869 - acc: 0.89125\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "51/60 [========================>.....] - ETA: 38s - loss: 2.1831 - acc: 0.89505\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "59/60 [============================>.] - ETA: 4s - loss: 2.1769 - acc: 0.90135\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "60/60 [==============================] - 258s 4s/step - loss: 2.1761 - acc: 0.9020\n",
      "Epoch 24/40\n",
      " 9/60 [===>..........................] - ETA: 3:40 - loss: 2.1277 - acc: 0.95055\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "19/60 [========>.....................] - ETA: 2:56 - loss: 2.1687 - acc: 0.90955\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "31/60 [==============>...............] - ETA: 2:05 - loss: 2.1735 - acc: 0.90475\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "39/60 [==================>...........] - ETA: 1:30 - loss: 2.1755 - acc: 0.90275\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "49/60 [=======================>......] - ETA: 47s - loss: 2.1776 - acc: 0.90055\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "59/60 [============================>.] - ETA: 4s - loss: 2.1775 - acc: 0.90065\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "60/60 [==============================] - 258s 4s/step - loss: 2.1774 - acc: 0.9007\n",
      "Epoch 25/40\n",
      "11/60 [====>.........................] - ETA: 3:31 - loss: 2.2019 - acc: 0.87625\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "19/60 [========>.....................] - ETA: 2:56 - loss: 2.1887 - acc: 0.88945\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "29/60 [=============>................] - ETA: 2:13 - loss: 2.1830 - acc: 0.89515\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "39/60 [==================>...........] - ETA: 1:30 - loss: 2.1800 - acc: 0.89825\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "49/60 [=======================>......] - ETA: 47s - loss: 2.1825 - acc: 0.89575\n",
      "50/60 [========================>.....] - ETA: 43s - loss: 2.1818 - acc: 0.8964(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "59/60 [============================>.] - ETA: 4s - loss: 2.1806 - acc: 0.89765\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "60/60 [==============================] - 258s 4s/step - loss: 2.1804 - acc: 0.8978\n",
      "Epoch 26/40\n",
      "11/60 [====>.........................] - ETA: 3:30 - loss: 2.1370 - acc: 0.94115\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "19/60 [========>.....................] - ETA: 2:56 - loss: 2.1517 - acc: 0.92655\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "31/60 [==============>...............] - ETA: 2:04 - loss: 2.1635 - acc: 0.91465\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "41/60 [===================>..........] - ETA: 1:21 - loss: 2.1835 - acc: 0.89475\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "49/60 [=======================>......] - ETA: 47s - loss: 2.1836 - acc: 0.89465\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "59/60 [============================>.] - ETA: 4s - loss: 2.1855 - acc: 0.89265\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "60/60 [==============================] - 258s 4s/step - loss: 2.1856 - acc: 0.8926\n",
      "Epoch 27/40\n",
      " 9/60 [===>..........................] - ETA: 3:38 - loss: 2.1716 - acc: 0.90655\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "19/60 [========>.....................] - ETA: 2:55 - loss: 2.1855 - acc: 0.89265\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "29/60 [=============>................] - ETA: 2:12 - loss: 2.2123 - acc: 0.86585\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "39/60 [==================>...........] - ETA: 1:29 - loss: 2.2076 - acc: 0.87055\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "49/60 [=======================>......] - ETA: 47s - loss: 2.2047 - acc: 0.87355\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "60/60 [==============================] - 257s 4s/step - loss: 2.2099 - acc: 0.8682\n",
      "Epoch 28/40\n",
      " 1/60 [..............................] - ETA: 4:14 - loss: 2.1880 - acc: 0.89025\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      " 9/60 [===>..........................] - ETA: 3:37 - loss: 2.2150 - acc: 0.86315\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "21/60 [=========>....................] - ETA: 2:46 - loss: 2.2541 - acc: 0.82405\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "29/60 [=============>................] - ETA: 2:12 - loss: 2.2622 - acc: 0.81595\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "39/60 [==================>...........] - ETA: 1:29 - loss: 2.2485 - acc: 0.82965\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "51/60 [========================>.....] - ETA: 38s - loss: 2.2293 - acc: 0.84885\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "59/60 [============================>.] - ETA: 4s - loss: 2.2237 - acc: 0.85445\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "60/60 [==============================] - 257s 4s/step - loss: 2.2232 - acc: 0.8549\n",
      "Epoch 29/40\n",
      "10/60 [====>.........................] - ETA: 3:33 - loss: 2.1873 - acc: 0.89085\n",
      "11/60 [====>.........................] - ETA: 3:29 - loss: 2.1912 - acc: 0.8869(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "21/60 [=========>....................] - ETA: 2:47 - loss: 2.2160 - acc: 0.86225\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "31/60 [==============>...............] - ETA: 2:04 - loss: 2.2220 - acc: 0.85615\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "39/60 [==================>...........] - ETA: 1:29 - loss: 2.2165 - acc: 0.86175\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "51/60 [========================>.....] - ETA: 38s - loss: 2.2119 - acc: 0.86625\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "60/60 [==============================] - 257s 4s/step - loss: 2.2212 - acc: 0.8569\n",
      "Epoch 30/40\n",
      " 1/60 [..............................] - ETA: 4:18 - loss: 2.2050 - acc: 0.87305\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "11/60 [====>.........................] - ETA: 3:30 - loss: 2.2483 - acc: 0.82975\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "21/60 [=========>....................] - ETA: 2:47 - loss: 2.2530 - acc: 0.82505\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "31/60 [==============>...............] - ETA: 2:04 - loss: 2.2376 - acc: 0.84055\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "41/60 [===================>..........] - ETA: 1:21 - loss: 2.2192 - acc: 0.85895\n",
      "42/60 [====================>.........] - ETA: 1:17 - loss: 2.2192 - acc: 0.8589(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "49/60 [=======================>......] - ETA: 47s - loss: 2.2152 - acc: 0.8629[51, 41, 52, 57, 37, 49, 9, 46, 21, 55, 14, 20, 33, 60, 45, 30, 3, 22, 36, 40, 50, 17, 23, 58, 48, 43, 1, 5, 39, 16, 47, 24, 44, 35, 28, 25, 19, 15, 27, 54, 56, 12, 42, 7, 59, 10, 11, 6, 13, 18, 31, 38, 4, 26, 2, 34, 32, 8, 53, 29]\n",
      "5\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "59/60 [============================>.] - ETA: 4s - loss: 2.2175 - acc: 0.86065\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "60/60 [==============================] - 257s 4s/step - loss: 2.2167 - acc: 0.8614\n",
      "Epoch 31/40\n",
      "11/60 [====>.........................] - ETA: 3:29 - loss: 2.2277 - acc: 0.85035\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "19/60 [========>.....................] - ETA: 2:55 - loss: 2.2236 - acc: 0.85455\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "31/60 [==============>...............] - ETA: 2:04 - loss: 2.2123 - acc: 0.86585\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "39/60 [==================>...........] - ETA: 1:29 - loss: 2.2074 - acc: 0.87075\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "49/60 [=======================>......] - ETA: 47s - loss: 2.1994 - acc: 0.87875\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "59/60 [============================>.] - ETA: 4s - loss: 2.1964 - acc: 0.88175\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "60/60 [==============================] - 257s 4s/step - loss: 2.1962 - acc: 0.8819\n",
      "Epoch 32/40\n",
      " 9/60 [===>..........................] - ETA: 3:38 - loss: 2.1896 - acc: 0.88855\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "19/60 [========>.....................] - ETA: 2:55 - loss: 2.2106 - acc: 0.86755\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "29/60 [=============>................] - ETA: 2:12 - loss: 2.2170 - acc: 0.86105\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "41/60 [===================>..........] - ETA: 1:21 - loss: 2.2093 - acc: 0.86885\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "51/60 [========================>.....] - ETA: 38s - loss: 2.2017 - acc: 0.87645\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "60/60 [==============================] - 257s 4s/step - loss: 2.2011 - acc: 0.8770\n",
      "Epoch 33/40\n",
      " 1/60 [..............................] - ETA: 4:15 - loss: 2.1917 - acc: 0.88655\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      " 9/60 [===>..........................] - ETA: 3:38 - loss: 2.1628 - acc: 0.91545\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "19/60 [========>.....................] - ETA: 2:55 - loss: 2.1839 - acc: 0.89425\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "31/60 [==============>...............] - ETA: 2:04 - loss: 2.1861 - acc: 0.89205\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "39/60 [==================>...........] - ETA: 1:29 - loss: 2.2002 - acc: 0.87795\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "49/60 [=======================>......] - ETA: 47s - loss: 2.1956 - acc: 0.88255\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "59/60 [============================>.] - ETA: 4s - loss: 2.2069 - acc: 0.87125\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "60/60 [==============================] - 257s 4s/step - loss: 2.2093 - acc: 0.8688\n",
      "Epoch 34/40\n",
      " 9/60 [===>..........................] - ETA: 3:38 - loss: 2.1604 - acc: 0.91795\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "21/60 [=========>....................] - ETA: 2:47 - loss: 2.1746 - acc: 0.90365\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "31/60 [==============>...............] - ETA: 2:04 - loss: 2.1764 - acc: 0.90185\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "39/60 [==================>...........] - ETA: 1:29 - loss: 2.1724 - acc: 0.90585\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "49/60 [=======================>......] - ETA: 47s - loss: 2.1919 - acc: 0.88645\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "59/60 [============================>.] - ETA: 4s - loss: 2.1919 - acc: 0.88635\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "60/60 [==============================] - 257s 4s/step - loss: 2.1916 - acc: 0.8867\n",
      "Epoch 35/40\n",
      " 9/60 [===>..........................] - ETA: 3:38 - loss: 2.1854 - acc: 0.89275\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "21/60 [=========>....................] - ETA: 2:47 - loss: 2.1923 - acc: 0.88585\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "31/60 [==============>...............] - ETA: 2:04 - loss: 2.1901 - acc: 0.88815\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "39/60 [==================>...........] - ETA: 1:29 - loss: 2.1950 - acc: 0.88325\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "49/60 [=======================>......] - ETA: 47s - loss: 2.1952 - acc: 0.88305\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "59/60 [============================>.] - ETA: 4s - loss: 2.1838 - acc: 0.89445\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "60/60 [==============================] - 257s 4s/step - loss: 2.1830 - acc: 0.8952\n",
      "Epoch 36/40\n",
      " 9/60 [===>..........................] - ETA: 3:38 - loss: 2.2358 - acc: 0.84235\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "19/60 [========>.....................] - ETA: 2:55 - loss: 2.2355 - acc: 0.84295\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "29/60 [=============>................] - ETA: 2:12 - loss: 2.2183 - acc: 0.86005\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "39/60 [==================>...........] - ETA: 1:30 - loss: 2.2012 - acc: 0.87705\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "51/60 [========================>.....] - ETA: 38s - loss: 2.2083 - acc: 0.86995\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "59/60 [============================>.] - ETA: 4s - loss: 2.2073 - acc: 0.87095\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "60/60 [==============================] - 257s 4s/step - loss: 2.2070 - acc: 0.8712\n",
      "Epoch 37/40\n",
      " 9/60 [===>..........................] - ETA: 3:38 - loss: 2.2542 - acc: 0.82395\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "19/60 [========>.....................] - ETA: 2:55 - loss: 2.1936 - acc: 0.88465\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "29/60 [=============>................] - ETA: 2:12 - loss: 2.2080 - acc: 0.87015\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "41/60 [===================>..........] - ETA: 1:21 - loss: 2.2002 - acc: 0.87795\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "51/60 [========================>.....] - ETA: 38s - loss: 2.1966 - acc: 0.88155\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "60/60 [==============================] - 257s 4s/step - loss: 2.1917 - acc: 0.8865\n",
      "Epoch 38/40\n",
      " 1/60 [..............................] - ETA: 4:16 - loss: 2.1848 - acc: 0.89335\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "11/60 [====>.........................] - ETA: 3:29 - loss: 2.1878 - acc: 0.89035\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "21/60 [=========>....................] - ETA: 2:47 - loss: 2.1980 - acc: 0.88015\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "29/60 [=============>................] - ETA: 2:12 - loss: 2.2025 - acc: 0.87565\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "39/60 [==================>...........] - ETA: 1:29 - loss: 2.2117 - acc: 0.86645\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "51/60 [========================>.....] - ETA: 38s - loss: 2.2121 - acc: 0.86605\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "59/60 [============================>.] - ETA: 4s - loss: 2.2368 - acc: 0.84125\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "60/60 [==============================] - 257s 4s/step - loss: 2.2370 - acc: 0.8411\n",
      "Epoch 39/40\n",
      "11/60 [====>.........................] - ETA: 3:30 - loss: 2.1579 - acc: 0.92025\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "21/60 [=========>....................] - ETA: 2:46 - loss: 2.1785 - acc: 0.89965\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "31/60 [==============>...............] - ETA: 2:04 - loss: 2.2025 - acc: 0.87565\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "39/60 [==================>...........] - ETA: 1:29 - loss: 2.2015 - acc: 0.87665\n",
      "41/60 [===================>..........] - ETA: 1:21 - loss: 2.2025 - acc: 0.8756(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "51/60 [========================>.....] - ETA: 38s - loss: 2.1989 - acc: 0.87935\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "60/60 [==============================] - 257s 4s/step - loss: 2.2062 - acc: 0.8720\n",
      "Epoch 40/40\n",
      " 1/60 [..............................] - ETA: 4:16 - loss: 2.2133 - acc: 0.86485\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "11/60 [====>.........................] - ETA: 3:30 - loss: 2.1825 - acc: 0.89565\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "21/60 [=========>....................] - ETA: 2:47 - loss: 2.1718 - acc: 0.90655\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "31/60 [==============>...............] - ETA: 2:04 - loss: 2.1705 - acc: 0.90785\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "41/60 [===================>..........] - ETA: 1:21 - loss: 2.1795 - acc: 0.89875\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "49/60 [=======================>......] - ETA: 47s - loss: 2.1818 - acc: 0.8964[43, 35, 4, 2, 52, 40, 36, 55, 33, 28, 49, 19, 31, 11, 59, 34, 13, 24, 44, 60, 17, 22, 53, 20, 54, 30, 8, 5, 41, 6, 26, 7, 3, 10, 1, 57, 32, 50, 47, 58, 15, 16, 51, 42, 38, 39, 37, 46, 29, 9, 21, 18, 27, 12, 25, 23, 45, 14, 56, 48]\n",
      "5\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "59/60 [============================>.] - ETA: 4s - loss: 2.1965 - acc: 0.88175\n",
      "(10, 5, 360, 640, 22)   (10, 5, 360, 640, 1)\n",
      "60/60 [==============================] - 257s 4s/step - loss: 2.1980 - acc: 0.8802\n"
     ]
    }
   ],
   "source": [
    "seq.fit_generator(gen_mit_data(dataset_size=60, batch_size=1), steps_per_epoch=60, epochs = 40)\n",
    "seq.save('mit_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test purpose\n",
    "for x, y in gen_mit_data(dataset_size=1, batch_size=1):\n",
    "    print(x.shape, \" \", y.shape)\n",
    "    break\n",
    "print(x.shape, y.shape)\n",
    "seq.fit(x,y, batch_size=1, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown entry in loss dictionary: \"class_name\". Only expected the following keys: ['conv3d_1']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-eb282cc744f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# load model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mit_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m for x, y in gen_mit_data('/home/ubuntu/mit_driveseg_dataset/npz_files', batch_size=1,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deserialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mopened_new_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36m_deserialize_model\u001b[0;34m(f, custom_objects, compile)\u001b[0m\n\u001b[1;32m    310\u001b[0m                       \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                       \u001b[0mloss_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                       sample_weight_mode=sample_weight_mode)\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;31m# Set optimizer weights.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m                                      \u001b[0;34m'dictionary: \"'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\". '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                                      \u001b[0;34m'Only expected the following keys: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                                      str(self.output_names))\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0mloss_functions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown entry in loss dictionary: \"class_name\". Only expected the following keys: ['conv3d_1']"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# load model\n",
    "seq = load_model('mit_model.h5')\n",
    "\n",
    "for x, y in gen_mit_data('/home/ubuntu/mit_driveseg_dataset/npz_files', batch_size=1,\n",
    "                          start_idx = 84, dataset_size=1):\n",
    "    print(x.shape, \" \", y.shape)\n",
    "    break                        \n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "grid_spec = gridspec.GridSpec(2, 3, width_ratios=[6, 6, 2])\n",
    "\n",
    "plt.subplot(grid_spec[0])\n",
    "plt.imshow(x[0,0][:,:,:3].astype(int))\n",
    "plt.axis('off')\n",
    "plt.title('input frame {}'.format(0))\n",
    "\n",
    "plt.subplot(grid_spec[1])\n",
    "plt.imshow(np.argmax(x[0,0][:,:,3:], axis=2), alpha=1)\n",
    "plt.axis('off')\n",
    "plt.title('input frame {}'.format(0))\n",
    "\n",
    "plt.subplot(grid_spec[3])\n",
    "plt.imshow(seg_elevation.label_to_color_image(np.argmax(y[0,0], axis=2)))\n",
    "plt.title('gt frame {}'.format(0))\n",
    "plt.axis('off')\n",
    "\n",
    "\n",
    "seg_maps = seq.predict(x[:1])\n",
    "print(seg_maps.shape)\n",
    "\n",
    "plt.subplot(grid_spec[4])\n",
    "plt.imshow(seg_elevation.label_to_color_image(np.argmax(seg_maps[0,0], axis=2)))\n",
    "plt.axis('off')\n",
    "plt.title('pred frame {}'.format(0))\n",
    "\n",
    "ax = plt.subplot(grid_spec[2])\n",
    "plt.imshow(seg_elevation.FULL_COLOR_MAP[seg_elevation.LABELS_TO_COMP].astype(np.uint8), interpolation='nearest')\n",
    "ax.yaxis.tick_right()\n",
    "plt.yticks(range(len(seg_elevation.LABELS_TO_COMP)), seg_elevation.LABEL_NAMES[seg_elevation.LABELS_TO_COMP])\n",
    "plt.xticks([], [])\n",
    "ax.tick_params(width=0.0)\n",
    "plt.grid('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1/1 [==============================] - 4s 4s/step - loss: 2.2743 - acc: 0.8761\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 4s 4s/step - loss: 2.2480 - acc: 0.8808\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 4s 4s/step - loss: 2.2292 - acc: 0.8818\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 4s 4s/step - loss: 2.2079 - acc: 0.8924\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 4s 4s/step - loss: 2.1951 - acc: 0.8983\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 4s 4s/step - loss: 2.1878 - acc: 0.9003\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 4s 4s/step - loss: 2.1826 - acc: 0.9018\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 4s 4s/step - loss: 2.1779 - acc: 0.9050\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 4s 4s/step - loss: 2.1748 - acc: 0.9067\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 4s 4s/step - loss: 2.1739 - acc: 0.9065\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 4s 4s/step - loss: 2.1710 - acc: 0.9088\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 4s 4s/step - loss: 2.1697 - acc: 0.9097\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 4s 4s/step - loss: 2.1674 - acc: 0.9119\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 4s 4s/step - loss: 2.1658 - acc: 0.9134\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 4s 4s/step - loss: 2.1644 - acc: 0.9146\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 4s 4s/step - loss: 2.1624 - acc: 0.9164\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 4s 4s/step - loss: 2.1617 - acc: 0.9172\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 4s 4s/step - loss: 2.1607 - acc: 0.9181\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 4s 4s/step - loss: 2.1596 - acc: 0.9191\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 4s 4s/step - loss: 2.1597 - acc: 0.9190\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f702d930cc0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq.fit(x[:1],y[:1], batch_size=1, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
